# -*- coding: utf-8 -*-
"""codefor prediction and metrics

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NdVcBvX8pQQL4QPFo7RS6srfWiPpkHpE
"""

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor
import numpy as np
from math import sqrt
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

url = "https://storage.googleapis.com/mbcc/datasets/us_chronic_disease_indicators.csv"
df = pd.read_csv(url)
null_counts = df.isnull().sum()
count_of_duplicates = len(df) - len(df.drop_duplicates())
df = df.drop('locationdesc', axis=1)
drop_df=df.dropna()
filtered_df = df[df['datavaluetype'] == 'Number']
final_filtered=filtered_df[(filtered_df.topic=="Cardiovascular Disease")]
final_filtered2=final_filtered.drop(['datavalueunit', 'highconfidencelimit', 'lowconfidencelimit'], axis=1)
one_hot_encoded_data = pd.get_dummies(final_filtered2, columns = ['locationabbr'])

data=pd.read_csv('final_filtered2.csv')
if all(data['yearstart'] == data['yearend']):
    data = data.drop('yearend', axis=1)

# Selecting potential features for encoding and imputation
categorical_features = ['datasource', 'question', 'stratificationcategory1', 'stratification1',
                        'topicid', 'questionid', 'datavaluetypeid', 'stratificationcategoryid1',
                        'stratificationid1', 'locationabbr']
numerical_features = ['yearstart']
target_variable = 'datavalue'

# Filtering the dataset for Cardiovascular Disease
data_cvd = data[data['topic'] == 'Cardiovascular Disease']

# Separating features and target variable
X = data_cvd[categorical_features + numerical_features]
y = data_cvd[target_variable]

# Imputers and encoders
numerical_transformer = SimpleImputer(strategy='mean')
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundling preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Applying the transformations
X_transformed = preprocessor.fit_transform(X)

# Splitting the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)

# Retraining the model with the transformed data
xgb_model = XGBRegressor(random_state=42)
xgb_model.fit(X_train, y_train)

# Making predictions and evaluating the model
xgb_predictions = xgb_model.predict(X_test)
xgb_mse = mean_squared_error(y_test, xgb_predictions)
xgb_rmse = sqrt(xgb_mse)
xgb_r2 = r2_score(y_test, xgb_predictions)

# Convert state codes to numbers based on the provided dictionary
state_to_digit = {
    'AL': '01', 'AK': '02', 'AZ': '03', 'AR': '04', 'CA': '05',
    'CO': '06', 'CT': '07', 'DE': '08', 'FL': '09', 'GA': '10',
    'HI': '11', 'ID': '12', 'IL': '13', 'IN': '14', 'IA': '15',
    'KS': '16', 'KY': '17', 'LA': '18', 'ME': '19', 'MD': '20',
    'MA': '21', 'MI': '22', 'MN': '23', 'MS': '24', 'MO': '25',
    'MT': '26', 'NE': '27', 'NV': '28', 'NH': '29', 'NJ': '30',
    'NM': '31', 'NY': '32', 'NC': '33', 'ND': '34', 'OH': '35',
    'OK': '36', 'OR': '37', 'PA': '38', 'RI': '39', 'SC': '40',
    'SD': '41', 'TN': '42', 'TX': '43', 'UT': '44', 'VT': '45',
    'VA': '46', 'WA': '47', 'WV': '48', 'WI': '49', 'WY': '50'
}

placeholder_value = 'common'  #just to make it easy instead of giving all feilds.

# Ask the user for a state code
user_state_code = input("Enter a state code (e.g., 'CA' for California): ")

# Check if the state abbreviation is valid
if user_state_code in state_to_digit:
    # Update the template with the user's state code and the prediction year
    template_data = pd.DataFrame([{
        'yearstart': 2023,
        'datasource': placeholder_value,
        'question': placeholder_value,
        'stratificationcategory1': placeholder_value,
        'stratification1': placeholder_value,
        'topicid': placeholder_value,
        'questionid': placeholder_value,
        'datavaluetypeid': placeholder_value,
        'stratificationcategoryid1': placeholder_value,
        'stratificationid1': placeholder_value,
        'locationabbr': user_state_code  # Use the user-provided state code
    }])

    # Apply the state to digit mapping and preprocess
    template_data['location_2digit'] = template_data['locationabbr'].map(state_to_digit)
    template_data['location_2digit'] = pd.to_numeric(template_data['location_2digit'])

    # Preprocess the input
    X_predict_transformed = preprocessor.transform(template_data)

    # Make prediction using the model
    future_prediction_xgb = xgb_model.predict(X_predict_transformed)

    print(f"XGB Predicted data value for 2023 in {user_state_code}: {future_prediction_xgb[0]}")
else:
    print(f"State code '{user_state_code}' is not recognized. Please enter a valid state code.")
xgb_rmse, xgb_mse, xgb_r2

#other models and metrics
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor
import numpy as np
from math import sqrt
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression

url = "https://storage.googleapis.com/mbcc/datasets/us_chronic_disease_indicators.csv"
df = pd.read_csv(url)
null_counts = df.isnull().sum()
count_of_duplicates = len(df) - len(df.drop_duplicates())
df = df.drop('locationdesc', axis=1)
drop_df=df.dropna()
filtered_df = df[df['datavaluetype'] == 'Number']
final_filtered=filtered_df[(filtered_df.topic=="Cardiovascular Disease")]
final_filtered2=final_filtered.drop(['datavalueunit', 'highconfidencelimit', 'lowconfidencelimit'], axis=1)
one_hot_encoded_data = pd.get_dummies(final_filtered2, columns = ['locationabbr'])
final_filtered2.to_csv('final_filtered2.csv', index=False)

data=pd.read_csv('final_filtered2.csv')
if all(data['yearstart'] == data['yearend']):
    data = data.drop('yearend', axis=1)

# Selecting potential features for encoding and imputation
categorical_features = ['datasource', 'question', 'stratificationcategory1', 'stratification1',
                        'topicid', 'questionid', 'datavaluetypeid', 'stratificationcategoryid1',
                        'stratificationid1', 'locationabbr']
numerical_features = ['yearstart']
target_variable = 'datavalue'

# Filtering the dataset for Cardiovascular Disease
data_cvd = data[data['topic'] == 'Cardiovascular Disease']

# Separating features and target variable
X = data_cvd[categorical_features + numerical_features]
y = data_cvd[target_variable]

# Imputers and encoders
numerical_transformer = SimpleImputer(strategy='mean')
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundling preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Applying the transformations
X_transformed = preprocessor.fit_transform(X)

# Splitting the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)


lr_model = LinearRegression()
rf_model = RandomForestRegressor(random_state=42)

# Train the Linear Regression model
lr_model.fit(X_train, y_train)

# Train the Random Forest model
rf_model.fit(X_train, y_train)

# Make predictions with Linear Regression
lr_predictions = lr_model.predict(X_test)

# Make predictions with Random Forest
rf_predictions = rf_model.predict(X_test)

# Evaluate the Linear Regression model
lr_mse = mean_squared_error(y_test, lr_predictions)
lr_rmse = sqrt(lr_mse)
lr_r2 = r2_score(y_test, lr_predictions)

# Evaluate the Random Forest model
rf_mse = mean_squared_error(y_test, rf_predictions)
rf_rmse = sqrt(rf_mse)
rf_r2 = r2_score(y_test, rf_predictions)

print("Linear Regression - RMSE:", lr_rmse)
print("Linear Regression - MSE:", lr_mse, ", R2:", lr_r2)

print("Random Forest - RMSE:", rf_rmse)
print("Random Forest - MSE:", rf_mse, ", R2:", rf_r2)

# Retraining the model with the transformed data
xgb_model = XGBRegressor(random_state=42)
xgb_model.fit(X_train, y_train)

# Making predictions and evaluating the model
xgb_predictions = xgb_model.predict(X_test)
xgb_mse = mean_squared_error(y_test, xgb_predictions)
xgb_rmse = sqrt(xgb_mse)
xgb_r2 = r2_score(y_test, xgb_predictions)

# Convert state codes to numbers based on the provided dictionary
state_to_digit = {
    'AL': '01', 'AK': '02', 'AZ': '03', 'AR': '04', 'CA': '05',
    'CO': '06', 'CT': '07', 'DE': '08', 'FL': '09', 'GA': '10',
    'HI': '11', 'ID': '12', 'IL': '13', 'IN': '14', 'IA': '15',
    'KS': '16', 'KY': '17', 'LA': '18', 'ME': '19', 'MD': '20',
    'MA': '21', 'MI': '22', 'MN': '23', 'MS': '24', 'MO': '25',
    'MT': '26', 'NE': '27', 'NV': '28', 'NH': '29', 'NJ': '30',
    'NM': '31', 'NY': '32', 'NC': '33', 'ND': '34', 'OH': '35',
    'OK': '36', 'OR': '37', 'PA': '38', 'RI': '39', 'SC': '40',
    'SD': '41', 'TN': '42', 'TX': '43', 'UT': '44', 'VT': '45',
    'VA': '46', 'WA': '47', 'WV': '48', 'WI': '49', 'WY': '50'
}

placeholder_value = 'common'  # Replace this with actual common values if known

# Ask the user for a state code
user_state_code = input("Enter a state code (e.g., 'CA' for California): ")

# Check if the state abbreviation is valid
if user_state_code in state_to_digit:
    # Update the template with the user's state code and the prediction year
    template_data = pd.DataFrame([{
        'yearstart': 2023,
        'datasource': placeholder_value,
        'question': placeholder_value,
        'stratificationcategory1': placeholder_value,
        'stratification1': placeholder_value,
        'topicid': placeholder_value,
        'questionid': placeholder_value,
        'datavaluetypeid': placeholder_value,
        'stratificationcategoryid1': placeholder_value,
        'stratificationid1': placeholder_value,
        'locationabbr': user_state_code  # Use the user-provided state code
    }])

    # Apply the state to digit mapping and preprocess
    template_data['location_2digit'] = template_data['locationabbr'].map(state_to_digit)
    template_data['location_2digit'] = pd.to_numeric(template_data['location_2digit'])

    # Preprocess the input
    X_predict_transformed = preprocessor.transform(template_data)

    # Make prediction using the model
    future_prediction_xgb = xgb_model.predict(X_predict_transformed)

    print(f"XGB Predicted data value for 2023 in {user_state_code}: {future_prediction_xgb[0]}")
else:
    print(f"State code '{user_state_code}' is not recognized. Please enter a valid state code.")
xgb_rmse, xgb_mse, xgb_r2